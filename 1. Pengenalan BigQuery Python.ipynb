{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menjalankan BigQuery Python di Jupyter Notebook\n",
    "\n",
    "BigQuery adalah gudang data ( data warehouse ) yang tidak memiliki server ( serverless ), dapat mengkueri data dalam skala petabyte, dirancang untuk membuat semua analis data anda produktif dengan kinerja tinggi dengan harga yang relatif murah. Karena tidak ada infrastruktur untuk dikelola, jadi kita dapat fokus menganalisis data untuk menemukan wawasan yang bermakna menggunakan SQL tanpa perlu administrator database.\n",
    "\n",
    "BigQuery memungkinkan anda untuk menganalisis data secara realtime menggunakan kemampuan streaming data sehingga menghasilkan wawasan selalu terkini dan gratis hingga 1 TB data untuk kueri setiap bulan dan 10 GB data yang disimpan.\n",
    "\n",
    "Untuk menggunakan BigQuery bagi permula, Google menawarkan 2 opsi yaitu menggunakan Sandbox dan Gratis uji coba. Dalam tutorial ini kita akan menggunakan versi Sandbox. Jika anda sudah mendaftar versi uji coba atau bahkan berbayar juga dapat mengikuti tutorial ini.\n",
    "\n",
    "## Prasyarat\n",
    "\n",
    "1. Mendaftar BigQuery https://cloud.google.com/bigquery/\n",
    "2. Aktifkan [BigQuery API](https://console.cloud.google.com/flows/enableapi?apiid=bigquery&_ga=2.260443277.-1046273168.1553369088)\n",
    "\n",
    "Daftar Isi Konten :\n",
    "\n",
    "1. [Install Library ](#1.-Install-library)\n",
    "2. [Otentikasi menggunakan Service Account](#2.-Otentikasi-menggunakan-Service-Account)\n",
    "3. [Mengelola Datasets](#3.-Mengelola-dataset)\n",
    "4. [Mengelola Tabel](#4.-Mengelola-tabel)\n",
    "5. [Menjalankan Kueri Data](#5.-Menjalankan-Kueri-Data)\n",
    "6. [Memanage Jobs](#6.-Memanage-Jobs)\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otentikasi menggunakan Service Account\n",
    "\n",
    "Menggunakan kredensial Service Account sangat berguna saat bekerja pada remote server tanpa akses ke input pengguna.\n",
    "\n",
    "* Masuk ke https://console.cloud.google.com/apis/credentials/serviceaccountkey\n",
    "\n",
    "<img width=\"auto\" height=\"auto\" src=\"https://storage.googleapis.com/tommypratama/imgcdn/Screen%20Shot%202019-03-25%20at%2002.58.55.png\">\n",
    "\n",
    "* Kemudian pada **Service account** pilih **New service account**\n",
    "* **Service account name** : isi dengan nama yang unik\n",
    "* Pada bagian **Role** pilih **BigQuery > BigQuery Admin**\n",
    "* **Key type** menggunakan format JSON\n",
    "* Klik tombol **Create** dan simpan file\n",
    "* Ganti nama file menjadi **key.json** ( Ini hanya opsi untuk memudahkan melakukan load file saja, jika tidak dirubah juga tidak masalah) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modul bigquery dan setting kredensial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modul bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Jika kredensial tidak di simpan ke dalam variabel client\n",
    "# maka library client akan mencari kredensial dari project yang terhubung sekarang\n",
    "## client = bigquery.Client()\n",
    "\n",
    "# Setting kredensial menggunakan Service Account\n",
    "client = bigquery.Client.from_service_account_json('key.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mengelola dataset\n",
    "\n",
    "### Membuat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan project dataset id\n",
    "dataset_id = 'my_dataset'\n",
    "\n",
    "# Buat DatasetReference menggunakan ID dataset yang dipilih.\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "# Buat objek Dataset lengkap untuk dikirim ke API.\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "# Tentukan lokasi geografis di mana dataset harus berada.\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# Kirim dataset ke API untuk dibuat.\n",
    "# Meningkatkan google.api_core.exceptions.Conflict jika Dataset sudah siap\n",
    "# Dan ada dalam project \n",
    "dataset = client.create_dataset(dataset)  # API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang coba lihat pada halaman https://console.cloud.google.com/bigquery maka akan muncul datasets dengan nama **my_datasets** .\n",
    "\n",
    "\n",
    "<img align=\"left\" width=\"700\" height=\"700\" src=\"https://storage.googleapis.com/tommypratama/imgcdn/Screen%20Shot%202019-03-25%20at%2003.34.21.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atau bisa menggunakan perintah berikut ini untuk melihat daftar datasets yang ada dalam project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project bigquery-sandbox-235419:\n",
      "\tmy_dataset\n"
     ]
    }
   ],
   "source": [
    "datasets = list(client.list_datasets())\n",
    "project = client.project\n",
    "\n",
    "if datasets:\n",
    "    print('Datasets in project {}:'.format(project))\n",
    "    for dataset in datasets:  # API request(s)\n",
    "        print('\\t{}'.format(dataset.dataset_id))\n",
    "else:\n",
    "    print('{} project does not contain any datasets.'.format(project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melihat informasi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: my_dataset\n",
      "Description: None\n",
      "Labels:\n",
      "\tDataset has no labels defined.\n",
      "Tables:\n",
      "\tThis dataset does not contain any tables.\n"
     ]
    }
   ],
   "source": [
    "# Nama dataset\n",
    "dataset_id = 'my_dataset'\n",
    "\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "dataset = client.get_dataset(dataset_ref)  # API request\n",
    "\n",
    "# Lihat properti dataset\n",
    "print('Dataset ID: {}'.format(dataset_id))\n",
    "print('Description: {}'.format(dataset.description))\n",
    "print('Labels:')\n",
    "labels = dataset.labels\n",
    "if labels:\n",
    "    for label, value in labels.items():\n",
    "        print('\\t{}: {}'.format(label, value))\n",
    "else:\n",
    "    print(\"\\tDataset tidak memiliki label yang ditentukan.\")\n",
    "\n",
    "# Lihat tabel di dataset\n",
    "print('Tables:')\n",
    "tables = list(client.list_tables(dataset_ref))  # API request(s)\n",
    "if tables:\n",
    "    for table in tables:\n",
    "        print('\\t{}'.format(table.table_id))\n",
    "else:\n",
    "    print('\\tDataset ini tidak memiliki tabel apa pun.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mengelola tabel\n",
    "\n",
    "### Buat tabel kosong dengan skema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset('my_dataset')\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField('full_name', 'STRING', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('age', 'INTEGER', mode='REQUIRED'),\n",
    "]\n",
    "table_ref = dataset_ref.table('my_table')\n",
    "table = bigquery.Table(table_ref, schema=schema)\n",
    "table = client.create_table(table)  # API request\n",
    "\n",
    "assert table.table_id == 'my_table'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masukkan baris ke data tabel \n",
    "\n",
    "Menggunakan metode `` insert_rows() `` :\n",
    "\n",
    "```sql\n",
    "dataset_id = 'my_dataset'  # replace with your dataset ID\n",
    "# For this sample, the table must already exist and have a defined schema\n",
    "table_id = 'my_table'  # replace with your table ID\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "table = client.get_table(table_ref)  # API request\n",
    "\n",
    "rows_to_insert = [(u\"Phred Phlyntstone\", 32), (u\"Wylma Phlyntstone\", 29)]\n",
    "\n",
    "errors = client.insert_rows(table, rows_to_insert)  # API request\n",
    "\n",
    "assert errors == []\n",
    "```\n",
    "\n",
    "> **Catatan** : Untuk melakukan streaming data ( insert ) tidak berlaku untuk **BigQuery Sandbox**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat tabel dari hasil kueri ke tabel permanen\n",
    "\n",
    "BigQuery menyimpan semua hasil kueri ke tabel, yang bisa permanen atau sementara:\n",
    "\n",
    "* Tabel sementara adalah tabel yang diberi nama secara acak yang disimpan dalam dataset khusus. Tabel sementara digunakan untuk [men-cache](https://cloud.google.com/bigquery/docs/cached-results) hasil kueri. Tabel sementara memiliki batas waktu sekitar 24 jam. Tabel sementara tidak tersedia untuk dibagikan, dan tidak terlihat menggunakan daftar standar atau metode manipulasi tabel lainnya. Anda tidak dikenai biaya untuk menyimpan tabel sementara.\n",
    "\n",
    "* Tabel permanen dapat berupa tabel baru atau yang sudah ada dalam dataset yang anda akses. Jika anda menulis hasil kueri ke tabel baru, Anda akan dikenakan biaya untuk menyimpan data. Saat anda menulis hasil kueri ke tabel permanen, tabel yang anda kueri harus berada di lokasi yang sama dengan dataset yang berisi tabel tujuan.\n",
    "\n",
    "Saat Anda menulis hasil kueri ke tabel permanen, anda bisa membuat tabel baru, menambahkan hasilnya ke tabel yang ada, atau menimpa tabel yang sudah ada.\n",
    "\n",
    "Lihat dokumentasi BigQuery untuk informasi lebih lanjut tentang [menulis hasil kueri](https://cloud.google.com/bigquery/docs/writing-results) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/bigquery-sandbox-235419/datasets/my_dataset/tables/corpus\n"
     ]
    }
   ],
   "source": [
    "# Tentukan tujuan dataset_id\n",
    "dataset_id = 'my_dataset'\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "# Atur tabel tujuan\n",
    "table_ref = client.dataset(dataset_id).table(\"corpus\")\n",
    "job_config.destination = table_ref\n",
    "sql = \"\"\"\n",
    "    SELECT corpus\n",
    "    FROM `bigquery-public-data.samples.shakespeare`\n",
    "    GROUP BY corpus;\n",
    "\"\"\"\n",
    "\n",
    "# Mulai kueri, melewati konfigurasi tambahan.\n",
    "query_job = client.query(\n",
    "    sql,\n",
    "    # Lokasi harus cocok dengan dataset yang dirujuk dalam kueri\n",
    "    # dan dari tabel tujuan.\n",
    "    location=\"US\",\n",
    "    job_config=job_config,\n",
    ")  # Permintaan API - memulai kueri\n",
    "\n",
    "query_job.result()  # Menunggu permintaan kueri selesai\n",
    "print(\"Query results loaded to table {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menyimpan hasil kueri data yang besar menggunakan legacy SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/bigquery-sandbox-235419/datasets/my_dataset/tables/corpus2\n"
     ]
    }
   ],
   "source": [
    "# Nama dataset\n",
    "dataset_id = 'my_dataset'\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "# Set use_legacy_sql ke True untuk menggunakan sintaks legacy SQL.\n",
    "job_config.use_legacy_sql = True\n",
    "# Atur tabel tujuan\n",
    "table_ref = client.dataset(dataset_id).table('corpus2')\n",
    "job_config.destination = table_ref\n",
    "job_config.allow_large_results = True\n",
    "sql = \"\"\"\n",
    "    SELECT corpus\n",
    "    FROM [bigquery-public-data:samples.shakespeare]\n",
    "    GROUP BY corpus;\n",
    "\"\"\"\n",
    "# Mulai kueri, melewati konfigurasi tambahan.\n",
    "query_job = client.query(\n",
    "    sql,\n",
    "    # Lokasi harus cocok dengan dataset yang dirujuk dalam kueri\n",
    "    # dan dari tabel tujuan.\n",
    "    location='US',\n",
    "    job_config=job_config)  # Permintaan API - memulai kueri\n",
    "\n",
    "query_job.result()  # Menunggu permintaan kueri selesai\n",
    "print('Query results loaded to table {}'.format(table_ref.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mendapatkan informasi tabel\n",
    "\n",
    "Melihat informasi yang ada pada tabel menggunakan metode `` get_table() `` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SchemaField('corpus', 'STRING', 'NULLABLE', None, ())]\n",
      "None\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Nama dataset\n",
    "dataset_id = 'my_dataset'\n",
    "# Nama tabel\n",
    "table_id = 'corpus'\n",
    "\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "table = client.get_table(table_ref)  # Permintaan API\n",
    "\n",
    "# Lihat tabel properti\n",
    "print(table.schema)\n",
    "print(table.description)\n",
    "print(table.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelajahi baris data dalam tabel dengan metode `` list_rows() `` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word             word_count       corpus           corpus_date      \n",
      "C                1                sonnets          0                \n",
      "L                1                sonnets          0                \n",
      "imaginary        1                sonnets          0                \n",
      "H                1                sonnets          0                \n",
      "relief           1                sonnets          0                \n",
      "W                1                sonnets          0                \n",
      "V                1                sonnets          0                \n",
      "advised          1                sonnets          0                \n",
      "grey             1                sonnets          0                \n",
      "X                1                sonnets          0                \n"
     ]
    }
   ],
   "source": [
    "dataset_ref = client.dataset(\"samples\", project=\"bigquery-public-data\")\n",
    "table_ref = dataset_ref.table(\"shakespeare\")\n",
    "table = client.get_table(table_ref)  # API call\n",
    "\n",
    "# Muat semua baris dari sebuah tabel\n",
    "rows = client.list_rows(table)\n",
    "assert len(list(rows)) == table.num_rows\n",
    "\n",
    "# Muat 10 baris pertama\n",
    "rows = client.list_rows(table, max_results=10)\n",
    "assert len(list(rows)) == 10\n",
    "\n",
    "# Tentukan bidang yang dipilih untuk membatasi hasil ke kolom tertentu\n",
    "fields = table.schema[:2]  # first two columns\n",
    "rows = client.list_rows(table, selected_fields=fields, max_results=10)\n",
    "assert len(rows.schema) == 2\n",
    "assert len(list(rows)) == 10\n",
    "\n",
    "# Gunakan indeks awal untuk memuat bagian tabel\n",
    "rows = client.list_rows(table, start_index=10, max_results=10)\n",
    "\n",
    "# Cetak baris data dalam format tabel\n",
    "format_string = \"{!s:<16} \" * len(rows.schema)\n",
    "field_names = [field.name for field in rows.schema]\n",
    "print(format_string.format(*field_names))  # prints column headers\n",
    "for row in rows:\n",
    "    print(format_string.format(*row))  # prints row data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melihat isi tabel yang ada pada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n",
      "\tcorpus\n",
      "\tcorpus2\n",
      "\tmy_table\n"
     ]
    }
   ],
   "source": [
    "# Nama dataset\n",
    "dataset_id = 'my_dataset'\n",
    "\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "dataset = client.get_dataset(dataset_ref)  # Permintaan API\n",
    "\n",
    "# Lihat tabel dalam dataset\n",
    "print('Tables:')\n",
    "tables = list(client.list_tables(dataset_ref))  # Permintaan API(s)\n",
    "if tables:\n",
    "    for table in tables:\n",
    "        print('\\t{}'.format(table.table_id))\n",
    "else:\n",
    "    print('\\tThis dataset does not contain any tables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data ke tabel BigQuery dari file lokal \n",
    "\n",
    "Menggunakan metode `` load_table_from_file() `` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 206 rows into my_dataset:countries.\n"
     ]
    }
   ],
   "source": [
    "filename = '_datasets/countries.csv'\n",
    "dataset_id = 'my_dataset'\n",
    "table_id = 'countries'\n",
    "\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.autodetect = True\n",
    "\n",
    "with open(filename, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(\n",
    "        source_file,\n",
    "        table_ref,\n",
    "        location=\"US\",  # Harus sama dengan lokasi dataset tujuan\n",
    "        job_config=job_config,\n",
    "    )  # Permintaan API\n",
    "\n",
    "job.result()  # Tunggu sampai kueri selesai\n",
    "\n",
    "print(\"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_id, table_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV data dari Google Cloud Storage menggunakan schema auto-detection\n",
    "\n",
    "Memuat file CSV dari Cloud Storage dengan metode `` load_table_from_uri() `` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job a5339517-5b89-4f7d-bfc2-2f4e1105a582\n",
      "Job finished.\n",
      "Loaded 50 rows.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 'my_dataset'\n",
    "table_id = 'us_states'\n",
    "uri = 'gs://cloud-samples-data/bigquery/us-states/us-states.csv'\n",
    "\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.autodetect = True\n",
    "job_config.skip_leading_rows = 1\n",
    "# Format sumber default ke CSV, jadi baris di bawah ini opsional.\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri,\n",
    "    dataset_ref.table(table_id),\n",
    "    job_config=job_config)  # Permintaan API\n",
    "print('Starting job {}'.format(load_job.job_id))\n",
    "\n",
    "load_job.result()  # Tunggu loading data selesai\n",
    "print('Job finished.')\n",
    "\n",
    "destination_table = client.get_table(dataset_ref.table(table_id))\n",
    "print('Loaded {} rows.'.format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menyalin tabel\n",
    "\n",
    "Salin tabel dengan metode `` copy_table() `` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = client.dataset(\"samples\", project=\"bigquery-public-data\")\n",
    "source_table_ref = source_dataset.table(\"shakespeare\")\n",
    "\n",
    "dataset_id = 'my_dataset'\n",
    "table_id = 'shakespeare'\n",
    "dest_table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "job = client.copy_table(\n",
    "    source_table_ref,\n",
    "    dest_table_ref,\n",
    "    # Lokasi harus cocok dengan tabel sumber dan tujuan.\n",
    "    location=\"US\",\n",
    ")  # Permintaan API\n",
    "\n",
    "job.result()  # Tunggu job sampai selesai\n",
    "\n",
    "assert job.state == \"DONE\"\n",
    "dest_table = client.get_table(dest_table_ref)  # Permintaan API\n",
    "assert dest_table.num_rows > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salin data tabel ke Google Cloud Storage dengan metode `` extract_table() `` :\n",
    "\n",
    "> **Catatan** : Untuk dapat mengupload tabel BigQuery ke Google Cloud Storage anda harus menambahkan permission menggunakan email yang digunakan `` Service Account `` pada file `` key.json ``\n",
    "\n",
    "1. Masuk ke https://console.cloud.google.com/storage/\n",
    "2. Klik nama bucket, jika anda belum membuat bucket bisa langsung membuatnya dengan **Create bucket**\n",
    "3. Pilih **Permission** > **Add members**\n",
    "4. Masukkan email yang ada pada file **key.json** di bagian **Members** ,kemudian pada bagian **Roles** pilih **Storage** > **Storage Admin** > **Add**\n",
    "\n",
    "<img align=\"left\" width=\"600\" height=\"600\" src=\"https://storage.googleapis.com/tommypratama/imgcdn/Screen%20Shot%202019-03-25%20at%2005.58.00.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported bigquery-public-data:samples.shakespeare to gs://tabel-bigquery/shakespeare.csv\n"
     ]
    }
   ],
   "source": [
    "# Nama bucket\n",
    "bucket_name = 'tabel-bigquery'\n",
    "# Atur project id, dataset id, tabel id\n",
    "project = \"bigquery-public-data\"\n",
    "dataset_id = \"samples\"\n",
    "table_id = \"shakespeare\"\n",
    "\n",
    "# Destinasi lokasi file penyimpanan Cloud Storage\n",
    "destination_uri = \"gs://{}/{}\".format(bucket_name, \"shakespeare.csv\")\n",
    "dataset_ref = client.dataset(dataset_id, project=project)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    # Lokasi harus sama dengan sumber tabel.\n",
    "    location=\"US\",\n",
    ")  # Permintaan API\n",
    "extract_job.result()  # tunggu job sampai dengan selesai.\n",
    "\n",
    "print(\n",
    "    \"Exported {}:{}.{} to {}\".format(project, dataset_id, table_id, destination_uri)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghapus tabel\n",
    "\n",
    "Hapus tabel dengan metode `` delete_table() `` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabel my_dataset:corpus2 berhasil dihapus.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 'my_dataset' # Ganti dengan dataset ID anda.\n",
    "table_id = 'corpus2'      # Ganti dengan table ID anda.\n",
    "\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "client.delete_table(table_ref)  # Permintaan API\n",
    "\n",
    "print('Tabel {}:{} berhasil dihapus.'.format(dataset_id, table_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Menjalankan Kueri Data\n",
    "\n",
    "Jalankan kueri sql berikut ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(('Frances',), {'name': 0})\n",
      "Row(('Alice',), {'name': 0})\n",
      "Row(('Beatrice',), {'name': 0})\n",
      "Row(('Ella',), {'name': 0})\n",
      "Row(('Gertrude',), {'name': 0})\n",
      "Row(('Josephine',), {'name': 0})\n",
      "Row(('Lula',), {'name': 0})\n",
      "Row(('Blanche',), {'name': 0})\n",
      "Row(('Marjorie',), {'name': 0})\n",
      "Row(('Christine',), {'name': 0})\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` \"\n",
    "    'WHERE state = \"TX\" '\n",
    "    \"LIMIT 10\"\n",
    ")\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Lokasi harus cocok dengan dataset yang dirujuk dalam kueri.\n",
    "    location=\"US\",\n",
    ")  # Permintaan API - memulai kueri\n",
    "\n",
    "for row in query_job:  # Permintaan API - mengambil hasil\n",
    "    # Nilai baris dapat diakses dengan nama field atau indeks\n",
    "    assert row[0] == row.name == row[\"name\"]\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jalankan dry run query\n",
    "\n",
    "**Dry run query** digunakan untuk melihat seberapa besar file yang akan di proses dalam kueri, sekaligus untuk mengontrol biaya ketika menggunakan kueri data. Jadi jika kita tidak tahu seberapa besar file yang akan di kueri sebaik nya menggunakan metode ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This query will process 65935918 bytes.\n"
     ]
    }
   ],
   "source": [
    "# from google.cloud import bigquery\n",
    "# client = bigquery.Client()\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.dry_run = True\n",
    "job_config.use_query_cache = False\n",
    "query_job = client.query(\n",
    "    (\n",
    "        \"SELECT name, COUNT(*) as name_count \"\n",
    "        \"FROM `bigquery-public-data.usa_names.usa_1910_2013` \"\n",
    "        \"WHERE state = 'WA' \"\n",
    "        \"GROUP BY name\"\n",
    "    ),\n",
    "    # Lokasi harus cocok dengan dataset yang dirujuk dalam kueri.\n",
    "    location=\"US\",\n",
    "    job_config=job_config,\n",
    ")  # Permintaan API\n",
    "\n",
    "# Permintaan dry run query selesai segera.\n",
    "assert query_job.state == \"DONE\"\n",
    "assert query_job.dry_run\n",
    "\n",
    "print(\"This query will process {} bytes.\".format(query_job.total_bytes_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menggunakan hasil query yang di-cache\n",
    "\n",
    "BigQuery menulis semua hasil kueri ke tabel. Tabel ini secara eksplisit diidentifikasi oleh pengguna (tabel tujuan), atau tabel hasil sementara yang di-cache. Tabel hasil sementara, di-cache dipertahankan per-pengguna, per-proyek. Tidak ada biaya penyimpanan untuk tabel sementara, tetapi jika Anda menulis hasil kueri ke tabel permanen, Anda akan dikenakan biaya untuk menyimpan data.\n",
    "\n",
    "Semua hasil kueri, termasuk kueri interaktif dan batch, di-cache dalam tabel sementara selama sekitar 24 jam dengan beberapa pengecualian.\n",
    "\n",
    "Dokumentasi https://cloud.google.com/bigquery/docs/cached-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(('sonnets',), {'corpus': 0})\n",
      "Row(('various',), {'corpus': 0})\n",
      "Row(('1kinghenryvi',), {'corpus': 0})\n",
      "Row(('2kinghenryvi',), {'corpus': 0})\n",
      "Row(('3kinghenryvi',), {'corpus': 0})\n"
     ]
    }
   ],
   "source": [
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.use_query_cache = True\n",
    "sql = \"\"\"\n",
    "    SELECT corpus\n",
    "    FROM `bigquery-public-data.samples.shakespeare`\n",
    "    GROUP BY corpus\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    sql,\n",
    "    # Lokasi harus cocok dengan dataset yang dirujuk dalam kueri.\n",
    "    location='US',\n",
    "    job_config=job_config)  # Permintaan API\n",
    "\n",
    "# Cetak hasilnya.\n",
    "for row in query_job:  # Permintaan API - mengambil hasil\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jalankan kueri menggunakan nama parameter kueri\n",
    "\n",
    "Lihat dokumentasi BigQuery untuk informasi lebih lanjut tentang [parameterized queries](https://cloud.google.com/bigquery/docs/parameterized-queries) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: \t614\n",
      "I: \t577\n",
      "and: \t490\n",
      "to: \t486\n",
      "a: \t407\n",
      "of: \t367\n",
      "my: \t314\n",
      "is: \t307\n",
      "in: \t291\n",
      "you: \t271\n",
      "that: \t270\n",
      "me: \t263\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT word, word_count\n",
    "    FROM `bigquery-public-data.samples.shakespeare`\n",
    "    WHERE corpus = @corpus\n",
    "    AND word_count >= @min_word_count\n",
    "    ORDER BY word_count DESC;\n",
    "\"\"\"\n",
    "query_params = [\n",
    "    bigquery.ScalarQueryParameter(\"corpus\", \"STRING\", \"romeoandjuliet\"),\n",
    "    bigquery.ScalarQueryParameter(\"min_word_count\", \"INT64\", 250),\n",
    "]\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.query_parameters = query_params\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Lokasi harus cocok dengan dataset yang dirujuk dalam kueri.\n",
    "    location=\"US\",\n",
    "    job_config=job_config,\n",
    ")  # Permintaan API - memulai kueri\n",
    "\n",
    "# Cetak hasil kueri\n",
    "for row in query_job:\n",
    "    print(\"{}: \\t{}\".format(row.word, row.word_count))\n",
    "\n",
    "assert query_job.state == \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memanage Jobs\n",
    "\n",
    "Jobs menggambarkan tindakan yang dilakukan pada data dalam tabel BigQuery:\n",
    "\n",
    "* Load data ke dalam table\n",
    "* Menjalankan kueri terhadap data dalam satu atau beberapa tabel\n",
    "* Ekstrak data dari tabel\n",
    "* Menyalin tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 jobs:\n",
      "7921ab17-170f-4421-8a1b-9c11b7353e80\n",
      "0a25a319-c374-4225-b05e-a2da2bc1cdce\n",
      "8a49e089-3231-4aed-bc86-b46dbc5a6489\n",
      "a5339517-5b89-4f7d-bfc2-2f4e1105a582\n",
      "29ea4f32-d9c6-41be-b7f6-7639db15f1b3\n",
      "0a6faeea-dfac-43c5-a930-24b23a7ea019\n",
      "2af00eff-1864-4b2a-9523-b2d5f332cd3f\n",
      "cdd0be0b-73ac-4fd4-a45f-491e759d72de\n",
      "159c7dfd-eeec-495e-963b-87c69140afcf\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Melihat daftar 10 Jobs terakhir\n",
    "# Hapus parameter max_result untuk melihat daftar jobs dari 6 bulan terakhir.\n",
    "print(\"10 jobs terakhir:\")\n",
    "for job in client.list_jobs(max_results=10):  # Permintaan API(s)\n",
    "    print(job.job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs from the last ten minutes:\n"
     ]
    }
   ],
   "source": [
    "# Berikut ini adalah contoh parameter opsional tambahan:\n",
    "# Gunakan min_creation_time atau max_creation_time untuk menentukan waktu\n",
    "print(\"Jobs yang berjalan dari 10 menit terakhir:\")\n",
    "ten_mins_ago = datetime.datetime.utcnow() - datetime.timedelta(minutes=10)\n",
    "for job in client.list_jobs(min_creation_time=ten_mins_ago):\n",
    "    print(job.job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 jobs terakhir yang dijalankan oleh semua pengguna:\n",
      "79ed98f6-3566-4f6c-b007-3b4c82c3de62 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "bb086bc0-7858-495c-bbae-b2cb6b027065 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "cf347992-de7e-47dd-a52f-12cf5898f56d dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "03f2ed34-e1c3-4bd6-b60b-2a62785412a2 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "4e24585e-9c7c-4e5f-88ec-b088b38e5727 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "29a865bf-cc26-4cd9-96cc-aa2dc25f0a1c dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "bde7979f-4e5b-4a0e-bf58-66e41bdbd178 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "93db7856-5d05-421a-bf79-c8a2b9b981c0 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "0abe756e-a87b-4edc-822c-8820133e8de4 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n",
      "84481cb9-0936-4b3a-8d91-45969aa9ef58 dijalankan oleh user: bigquery-free@bigquery-sandbox-235419.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Gunakan all_users untuk untuk melihat jobs yang dijalankan oleh semua pengguna dalam project.\n",
    "print(\"10 jobs terakhir yang dijalankan oleh semua pengguna:\")\n",
    "for job in client.list_jobs(max_results=10, all_users=True):\n",
    "    print(\"{} dijalankan oleh user: {}\".format(job.job_id, job.user_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs currently running:\n"
     ]
    }
   ],
   "source": [
    "# Memfilter jobs yang sedang berjalan menggunakan state.filter\n",
    "print(\"Jobs yang sedang berjalan:\")\n",
    "for job in client.list_jobs(state_filter=\"RUNNING\"):\n",
    "    print(job.job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sumber** :\n",
    "\n",
    "* https://googleapis.github.io/google-cloud-python/latest/bigquery/usage/queries.html\n",
    "* https://googleapis.github.io/google-cloud-python/latest/bigquery/usage/tables.html\n",
    "* https://googleapis.github.io/google-cloud-python/latest/bigquery/magics.html\n",
    "* https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
